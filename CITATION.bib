% ========================================
% UniPruning Citation
% ========================================

@inproceedings{unipruning2026,
  title={UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs},
  author={Anonymous},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2026},
  url={https://openreview.net},
  note={Under review}
}

% ========================================
% Alternative formats
% ========================================

% ArXiv version (to be updated upon publication)
@article{unipruning2026arxiv,
  title={UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs},
  author={Anonymous},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2026},
  url={https://arxiv.org/abs/XXXX.XXXXX}
}

% ========================================
% Related Work - openPangu
% ========================================

@misc{openpangu2023,
  title={openPangu: Open-source Large-scale Pre-trained Language Models},
  author={Pengcheng Laboratory},
  year={2023},
  howpublished={\url{https://openi.pcl.ac.cn/OpenPangu}},
  note={Accessed: 2025-12-09}
}

% ========================================
% Related Work - Base Models
% ========================================

@article{touvron2023llama,
  title={Llama: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{touvron2023llama2,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{dubey2024llama3,
  title={The Llama 3 Herd of Models},
  author={Grattafiori, Aaron and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{qwen2025,
  title={Qwen2.5: A Party of Foundation Models},
  author={Qwen Team},
  journal={arXiv preprint arXiv:2501.10650},
  year={2025}
}

@article{guo2025deepseek,
  title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
  author={Guo, Daya and Zhu, Qihao and Yang, Dejian and Xie, Zhenda and Dong, Kai and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

% ========================================
% Related Work - Pruning Methods
% ========================================

@inproceedings{sun2024wanda,
  title={A Simple and Effective Pruning Approach for Large Language Models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J. Zico},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}

@inproceedings{zhang2024ria,
  title={Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models},
  author={Zhang, Yingtao and Bai, Haoli and Lin, Haokun and Zhao, Jialin and Hou, Lu and Cannistraci, Carlo Vittorio},
  booktitle={The Twelfth International Conference on Learning Representations (ICLR)},
  year={2024}
}

@article{yi2025stochria,
  title={Symmetric Pruning of Large Language Models},
  author={Yi, Kai and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2501.18980},
  year={2025}
}

@article{frantar2023sparsegpt,
  title={SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot},
  author={Frantar, Elias and Alistarh, Dan},
  journal={arXiv preprint arXiv:2301.00774},
  year={2023}
}

@inproceedings{liu2025proxsparse,
  title={ProxSparse: Regularized Learning of Semi-Structured Sparsity Masks for Pretrained LLMs},
  author={Liu, Hongyi and Saha, Rajarshi and Jia, Zhen and Park, Youngsuk and Huang, Jiaji and Sabach, Shoham and Wang, Yu-Xiang and Karypis, George},
  booktitle={Proceedings of the 42nd International Conference on Machine Learning (ICML)},
  year={2025},
  organization={PMLR}
}

@inproceedings{bai2024sparsellm,
  title={SparseLLM: Towards Global Pruning of Pre-trained Language Models},
  author={Bai, Guangji and Li, Yijiang and Ling, Chen and Kim, Kibaek and Zhao, Liang},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2024}
}

% ========================================
% Related Work - Optimization Theory
% ========================================

@book{beck2003mirror,
  title={Mirror Descent and Nonlinear Projected Subgradient Methods for Convex Optimization},
  author={Beck, Amir and Teboulle, Marc},
  journal={Operations Research Letters},
  volume={31},
  number={3},
  pages={167--175},
  year={2003},
  publisher={Elsevier}
}

@book{nemirovsky1983problem,
  title={Problem Complexity and Method Efficiency in Optimization},
  author={Nemirovsky, Arkadi S. and Yudin, David B.},
  publisher={Wiley},
  address={New York},
  year={1983}
}

@book{bubeck2015convex,
  title={Convex Optimization: Algorithms and Complexity},
  author={Bubeck, S{\'e}bastien},
  journal={Foundations and Trends in Machine Learning},
  volume={8},
  number={3--4},
  pages={231--357},
  year={2015},
  publisher={Now Publishers Inc.}
}

@article{ding2025adaptive,
  title={Adaptive Pruning of Pretrained Transformer via Differential Inclusions},
  author={Ding, Yizhuo and Fan, Ke and Wang, Yikai and Sun, Xinwei and Fu, Yanwei},
  journal={arXiv preprint arXiv:2501.03289},
  year={2025}
}

% ========================================
% Evaluation and Benchmarks
% ========================================

@software{gao2024lm_eval,
  title={The Language Model Evaluation Harness},
  author={Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  month = {07},
  year = {2024},
  url = {https://zenodo.org/records/12608602}
}

@article{merity2016pointer,
  title={Pointer Sentinel Mixture Models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@article{raffel2020exploring,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

% ========================================
% Usage Example
% ========================================
%
% To cite this work in your paper, use:
%
% \cite{unipruning2026}
%
% For openPangu models:
%
% \cite{openpangu2023}
%
% Example sentence:
% "We employ UniPruning~\cite{unipruning2026} to prune openPangu
%  models~\cite{openpangu2023}, achieving XX\% sparsity while
%  maintaining YY accuracy."
%
% ========================================
